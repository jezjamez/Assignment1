{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa2ccab-9588-45c6-87db-ed196764fa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lmdb in ./pytorch-a1/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: fire in ./pytorch-a1/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: termcolor in ./pytorch-a1/lib/python3.10/site-packages (from fire) (2.0.1)\n",
      "Requirement already satisfied: six in ./pytorch-a1/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached numpy-1.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-1.23.3 opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lmdb\n",
    "# ! pip install fire\n",
    "# ! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8295e954-cae9-4dd7-bb64-7ecb809cf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import os\n",
    "import lmdb\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "601be921-8d38-42b1-ab09-b226f2f327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgH  = 224 \n",
    "imgW = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d90555ae-b81d-4a96-887d-027af69b5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkImageIsValid(imageBin):\n",
    "    if imageBin is None:\n",
    "        return False\n",
    "    imageBuf = np.frombuffer(imageBin, dtype=np.uint8)\n",
    "    img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)\n",
    "    imgH, imgW = img.shape[0], img.shape[1]\n",
    "    if imgH * imgW == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def writeCache(env, cache):\n",
    "    with env.begin(write=True) as txn:\n",
    "        for k, v in cache.items():\n",
    "            txn.put(k, v)\n",
    "\n",
    "\n",
    "def createDataset(inputPath = \"/home/jezjamez/work/Assignment1/data_lmdb_release/training/mjsynth/mnt/ramdisk/max/90kDICT32px/\",\n",
    "                  gtFile = \"/home/jezjamez/work/Assignment1/data_lmdb_release/training/mjsynth/mnt/ramdisk/max/90kDICT32px/annotation.txt\",\n",
    "                  outputPath = \"/home/jezjamez/work/Assignment1/data_lmdb_release/\", checkValid=True):\n",
    "\n",
    "    os.makedirs(outputPath, exist_ok=True)\n",
    "    env = lmdb.open(outputPath, map_size=1099511627776)\n",
    "    cache = {}\n",
    "    cnt = 1\n",
    "\n",
    "    with open(gtFile, 'r', encoding='utf-8') as data:\n",
    "        datalist = data.readlines()\n",
    "\n",
    "    nSamples = len(datalist)\n",
    "    for i in range(nSamples):\n",
    "        \n",
    "        imagePath, label = datalist[i].strip('\\n').split(' ')\n",
    "\n",
    "        imagePath = os.path.join(inputPath, imagePath)\n",
    "        if not os.path.exists(imagePath):\n",
    "            print('%s does not exist' % imagePath)\n",
    "            continue\n",
    "        with open(imagePath, 'rb') as f:\n",
    "            imageBin = f.read()\n",
    "        if checkValid:\n",
    "            try:\n",
    "                if not checkImageIsValid(imageBin):\n",
    "                    print('%s is not a valid image' % imagePath)\n",
    "                    continue\n",
    "            except:\n",
    "                print('error occured', i)\n",
    "                with open(outputPath + '/error_image_log.txt', 'a') as log:\n",
    "                    log.write('%s-th image data occured error\\n' % str(i))\n",
    "                continue\n",
    "\n",
    "        imageKey = 'image-%09d'.encode() % cnt\n",
    "        labelKey = 'label-%09d'.encode() % cnt\n",
    "        cache[imageKey] = imageBin\n",
    "        cache[labelKey] = label.encode()\n",
    "\n",
    "        if cnt % 1000 == 0:\n",
    "            writeCache(env, cache)\n",
    "            cache = {}\n",
    "            print('Written %d / %d' % (cnt, nSamples))\n",
    "        cnt += 1\n",
    "    nSamples = cnt-1\n",
    "    cache['num-samples'.encode()] = str(nSamples).encode()\n",
    "    writeCache(env, cache)\n",
    "    print('Created dataset with %d samples' % nSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a338e43-bcd0-4e67-a146-a53669d8bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire.Fire(createDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ac40007-ee2e-49bd-bc0a-54387e82b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LmdbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, ):\n",
    "\n",
    "        self.root = root\n",
    "        self.env = lmdb.open(root, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        if not self.env:\n",
    "            print('cannot create lmdb from %s' % (root))\n",
    "            sys.exit(0)\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            nSamples = int(txn.get('num-samples'.encode()))\n",
    "            self.nSamples = nSamples\n",
    "            \n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        index = self.filtered_index_list[index]\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "\n",
    "                img = Image.open(buf).convert('L')\n",
    "\n",
    "            except IOError:\n",
    "                print(f'Corrupted image for {index}')\n",
    "                img = Image.new('L', (imgW, imgH))\n",
    "                label = '[dummy_label]'\n",
    "\n",
    "            label = label.lower()\n",
    "            out_of_char = f'[^0123456789abcdefghijklmnopqrstuvwxyz]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "212dc416-fd84-4578-994a-8dca7abcb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls data_lmdb_release/mjsynth/mnt/ramdisk/max/90kDICT32px/298/1\n",
    "# lmdb.open(\"data_lmdb_release/\", max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61d33238-82b0-45d0-994d-552b31061ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=True):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        resized_max_w = self.imgW\n",
    "        input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "        transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "        resized_images = []\n",
    "        for image in images:\n",
    "            w, h = image.size\n",
    "            ratio = w / float(h)\n",
    "            if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                resized_w = self.imgW\n",
    "            else:\n",
    "                resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "            resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "            resized_images.append(transform(resized_image))\n",
    "\n",
    "        image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "\n",
    "        return image_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e1e3ed6a-3b10-46ab-9efa-70401b7ed612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, path =\"data_lmdb_release\"):\n",
    "        self.path = path\n",
    "        select_data = 'MJ'\n",
    "        imgH  = 224 \n",
    "        imgW = 224\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        _batch_size = 8\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=False)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        \n",
    "        dataset = LmdbDataset(path)\n",
    "        total_number_dataset = len(dataset)\n",
    "        print(total_number_dataset)\n",
    "        indices = range(total_number_dataset)\n",
    "        print(dataset)\n",
    "        batch_size_list.append(str(_batch_size))\n",
    "        Total_batch_size += _batch_size\n",
    "        _data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        collate_fn=_AlignCollate, pin_memory=True)\n",
    "\n",
    "        self.data_loader_list.append(_data_loader)\n",
    "        self.dataloader_iter_list.append(iter(_data_loader))\n",
    "        \n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image, text = data_loader_iter.next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = self.dataloader_iter_list[i].next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "72b886f7-9990-4d81-a753-8558872802b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6fbed0f9-8155-497c-9648-eba4858bd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLabelConverter:\n",
    "\n",
    "    def __init__(self):\n",
    "        character='0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        batch_max_length = 25\n",
    "        self.SPACE = '[s]'\n",
    "        self.GO = '[GO]'\n",
    "\n",
    "        self.list_token = [self.GO, self.SPACE]\n",
    "        self.character = self.list_token + list(character)\n",
    "\n",
    "        self.dict = {word: i for i, word in enumerate(self.character)}\n",
    "        self.batch_max_length = batch_max_length + len(self.list_token)\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        length = [len(s) + len(self.list_token) for s in text]  # +2 for [GO] and [s] at end of sentence.\n",
    "        batch_text = torch.LongTensor(len(text), self.batch_max_length).fill_(self.dict[self.GO])\n",
    "        for i, t in enumerate(text):\n",
    "            txt = [self.GO] + list(t) + [self.SPACE]\n",
    "            txt = [self.dict[char] for char in txt]\n",
    "            batch_text[i][:len(txt)] = torch.LongTensor(txt)  # batch_text[:, 0] = [GO] token\n",
    "        return batch_text.to(device)\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "50ad7f29-8846-4c20-a04d-be4c5794511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "converter = TokenLabelConverter()\n",
    "num_class = len(converter.character)\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8214af7e-2ba6-4c0c-bc86-b08b0e7bfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "from timm.models.registry import register_model\n",
    "from timm.models import create_model\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "038ec21f-6cc5-420d-a9dd-c2d6b6a890a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vis_model']\n",
    "\n",
    "def create_vitstr(num_tokens, model=\"vis_model\", checkpoint_path=''):\n",
    "    vitstr = create_model(\n",
    "        model,\n",
    "        pretrained=True,\n",
    "        num_classes=num_tokens,\n",
    "        checkpoint_path=checkpoint_path)\n",
    "\n",
    "    vitstr.reset_classifier(num_classes=num_tokens)\n",
    "\n",
    "    return vitstr\n",
    "\n",
    "class ViTSTR(VisionTransformer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def reset_classifier(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, seqlen: int =25):\n",
    "        x = self.forward_features(x)\n",
    "        x = x[:, :seqlen]\n",
    "\n",
    "        # batch, seqlen, embsize\n",
    "        b, s, e = x.size()\n",
    "        x = x.reshape(b*s, e)\n",
    "        x = self.head(x).view(b, s, self.num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_pretrained(model, cfg=None, num_classes=1000, in_chans=1, filter_fn=None, strict=True):\n",
    "\n",
    "    if cfg is None:\n",
    "        cfg = getattr(model, 'default_cfg')\n",
    "    if cfg is None or 'url' not in cfg or not cfg['url']:\n",
    "        _logger.warning(\"Pretrained model URL is invalid, using random initialization.\")\n",
    "        return\n",
    "\n",
    "    state_dict = model_zoo.load_url(cfg['url'], progress=True, map_location='cpu')\n",
    "    if \"model\" in state_dict.keys():\n",
    "        state_dict = state_dict[\"model\"]\n",
    "\n",
    "    if filter_fn is not None:\n",
    "        state_dict = filter_fn(state_dict)\n",
    "\n",
    "    if in_chans == 1:\n",
    "        conv1_name = cfg['first_conv']\n",
    "        key = conv1_name + '.weight'\n",
    "        if key in state_dict.keys():\n",
    "            conv1_weight = state_dict[conv1_name + '.weight']\n",
    "        else:\n",
    "            return\n",
    "        conv1_type = conv1_weight.dtype\n",
    "        conv1_weight = conv1_weight.float()\n",
    "        O, I, J, K = conv1_weight.shape\n",
    "        if I > 3:\n",
    "            assert conv1_weight.shape[1] % 3 == 0\n",
    "            conv1_weight = conv1_weight.reshape(O, I // 3, 3, J, K)\n",
    "            conv1_weight = conv1_weight.sum(dim=2, keepdim=False)\n",
    "        else:\n",
    "            conv1_weight = conv1_weight.sum(dim=1, keepdim=True)\n",
    "        conv1_weight = conv1_weight.to(conv1_type)\n",
    "        state_dict[conv1_name + '.weight'] = conv1_weight\n",
    "\n",
    "    classifier_name = cfg['classifier']\n",
    "    if num_classes == 1000 and cfg['num_classes'] == 1001:\n",
    "        classifier_weight = state_dict[classifier_name + '.weight']\n",
    "        state_dict[classifier_name + '.weight'] = classifier_weight[1:]\n",
    "        classifier_bias = state_dict[classifier_name + '.bias']\n",
    "        state_dict[classifier_name + '.bias'] = classifier_bias[1:]\n",
    "    elif num_classes != cfg['num_classes']:\n",
    "        del state_dict[classifier_name + '.weight']\n",
    "        del state_dict[classifier_name + '.bias']\n",
    "        strict = False\n",
    "\n",
    "    print(\"Loading pre-trained vision transformer weights from %s ...\" % cfg['url'])\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def vis_model(pretrained=False, **kwargs):\n",
    "    kwargs['in_chans'] = 1\n",
    "    kwargs.pop('pretrained_cfg', None)\n",
    "    print(kwargs)\n",
    "    model = ViTSTR(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True, **kwargs)\n",
    "    model.default_cfg = _cfg(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\"\n",
    "    )\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 1), filter_fn=_conv_filter)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e60bd3f2-aa74-4629-909c-7908bcd30cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Model, self).__init__()\n",
    "        self.vitstr = create_vitstr(num_tokens=num_class)\n",
    "        return\n",
    "\n",
    "    def forward(self, input, text, is_train=True, seqlen=25):\n",
    "\n",
    "\n",
    "        prediction = self.vitstr(input, seqlen=seqlen)\n",
    "        return prediction\n",
    "\n",
    "class JitModel(Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.vitstr= create_vitstr(num_tokens=opt.num_class)\n",
    "\n",
    "    def forward(self, input, seqlen:int = 25):\n",
    "        prediction = self.network(input, seqlen=seqlen)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "190ab234-d6fc-4da5-8024-896be768cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "8919257\n",
      "<__main__.LmdbDataset object at 0x7f37c02482b0>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Batch_Balanced_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7701f223-347d-4599-892b-5a45afdaa3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_classes': 96, 'in_chans': 1}\n",
      "Loading pre-trained vision transformer weights from https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth ...\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "309b239a-84e0-41c4-9ef3-6afd24003fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (vitstr): ViTSTR(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head): Linear(in_features=384, out_features=96, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model).to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "006ae766-a6fd-4ad8-bba9-005596c8c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        count = v.data.numel()\n",
    "        v = v.data.sum()\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7aebe09b-2f2e-4a48-a23c-6f4532568907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, evaluation_loader, converter):\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        image = image_tensors.to(device)\n",
    "\n",
    "        target = converter.encode(labels)\n",
    "\n",
    "        start_time = time.time()\n",
    "        preds = model(image, text=target, seqlen=converter.batch_max_length)\n",
    "        _, preds_index = preds.topk(1, dim=-1, largest=True, sorted=True)\n",
    "        preds_index = preds_index.view(-1, converter.batch_max_length)\n",
    "        forward_time = time.time() - start_time\n",
    "        cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "        length_for_pred = torch.IntTensor([converter.batch_max_length - 1] * batch_size).to(device)\n",
    "        preds_str = converter.decode(preds_index[:, 1:], length_for_pred)\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            \n",
    "            pred_EOS = pred.find('[s]')\n",
    "            pred = pred[:pred_EOS]  \n",
    "            pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "            \n",
    "            pred = pred.lower()\n",
    "            gt = gt.lower()\n",
    "            alphanumeric_case_insensitve = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "            out_of_alphanumeric_case_insensitve = f'[^{alphanumeric_case_insensitve}]'\n",
    "            pred = re.sub(out_of_alphanumeric_case_insensitve, '', pred)\n",
    "            gt = re.sub(out_of_alphanumeric_case_insensitve, '', gt)\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            if len(gt) == 0 or len(pred) == 0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  \n",
    "            confidence_score_list.append(confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data)  # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d3798f44-ff89-4e4c-942c-5950f14e40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "loss_avg = Averager()\n",
    "filtered_parameters = []\n",
    "params_num = []\n",
    "for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "    filtered_parameters.append(p)\n",
    "    params_num.append(np.prod(p.size()))\n",
    "    \n",
    "optimizer = optim.Adadelta(filtered_parameters, lr=1.0, rho=.95, eps=1e-8)\n",
    "scheduler = None\n",
    "best_accuracy = -1\n",
    "best_norm_ED = -1\n",
    "iteration = 0\n",
    "\n",
    "_AlignCollate = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=False)\n",
    "valid_dataset, valid_dataset_log = Batch_Balanced_Dataset(\"data_lmdb_release/valadation\")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=8,\n",
    "    shuffle=True,  \n",
    "    num_workers=int(2),\n",
    "    collate_fn=AlignCollate_valid, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ba72bd8-8f09-4617-9d56-1776a5e19969",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "        image_tensors, labels = train_dataset.get_batch()\n",
    "        image = image_tensors.to(device)\n",
    "    \n",
    "        batch_size = image.size(0)\n",
    "\n",
    "\n",
    "        target = converter.encode(labels)\n",
    "        preds = model(image, text=target, seqlen=converter.batch_max_length)\n",
    "        cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "        model.zero_grad()\n",
    "        cost.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5) \n",
    "        optimizer.step()\n",
    "\n",
    "        loss_avg.add(cost)\n",
    "        print(cost)\n",
    "        if (iteration + 1) % 2000 == 0 or iteration == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time, length_of_data = validation(\n",
    "                    model, criterion, valid_loader, converter)\n",
    "            model.train()\n",
    "\n",
    "            print(f'[{iteration+1}/2000] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}')\n",
    "            loss_avg.reset()\n",
    "            if current_accuracy > best_accuracy:\n",
    "                best_accuracy = current_accuracy\n",
    "                torch.save(model.state_dict(), f'./saved_models/Models/best_accuracy.pth')\n",
    "            if current_norm_ED > best_norm_ED:\n",
    "                best_norm_ED = current_norm_ED\n",
    "                torch.save(model.state_dict(), f'./saved_models/Models/best_norm_ED.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a486ab3-de09-4a79-b7b5-332eb7dd45f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c9c88-9fd5-4fdd-9728-8d5958109de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
